{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ece0a8ff-e7bb-4aa3-b33e-1a7c798962df",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Step 1: Stage Bronze - Ingest the data into Azure Databricks using Delta Lake to create the Bronze data store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f940eb21-72ed-4b42-b610-ba407b620992",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. Payment Tables\n",
    "payment_df = spark.read.format(\"csv\") \\\n",
    "    .option(\"inferSchema\", \"false\") \\\n",
    "    .option(\"header\", \"false\") \\\n",
    "    .option(\"sep\", \",\") \\\n",
    "    .load(\"/FileStore/Data/payments.csv\")\n",
    "\n",
    "\n",
    "#Renamed the column name \n",
    "payment_df = payment_df.withColumnRenamed('_c0','payment_id')\n",
    "payment_df = payment_df.withColumnRenamed('_c1','date')\n",
    "payment_df = payment_df.withColumnRenamed('_c2','amount')\n",
    "payment_df = payment_df.withColumnRenamed('_c3','rider_id')\n",
    "\n",
    "\n",
    "#Input Data to the Data store\n",
    "payment_df.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"mergeSchema\", \"true\") \\\n",
    "    .save(\"/delta/bronze/bronze_payments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "828d0a08-8d47-4141-a075-3fe98f3a2e07",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+------+--------+\n|payment_id|      date|amount|rider_id|\n+----------+----------+------+--------+\n|         1|2019-05-01|   9.0|    1000|\n|         2|2019-06-01|   9.0|    1000|\n|         3|2019-07-01|   9.0|    1000|\n|         4|2019-08-01|   9.0|    1000|\n|         5|2019-09-01|   9.0|    1000|\n|         6|2019-10-01|   9.0|    1000|\n|         7|2019-11-01|   9.0|    1000|\n|         8|2019-12-01|   9.0|    1000|\n|         9|2020-01-01|   9.0|    1000|\n|        10|2020-02-01|   9.0|    1000|\n|        11|2020-03-01|   9.0|    1000|\n|        12|2020-04-01|   9.0|    1000|\n|        13|2020-05-01|   9.0|    1000|\n|        14|2020-06-01|   9.0|    1000|\n|        15|2020-07-01|   9.0|    1000|\n|        16|2020-08-01|   9.0|    1000|\n|        17|2020-09-01|   9.0|    1000|\n|        18|2020-10-01|   9.0|    1000|\n|        19|2020-11-01|   9.0|    1000|\n|        20|2020-12-01|   9.0|    1000|\n+----------+----------+------+--------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "payment_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38ab814d-cc62-4f0e-ab17-6e4495924c1e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 2. Riders Tables\n",
    "riders_df = spark.read.format(\"csv\") \\\n",
    "    .option(\"inferSchema\", \"false\") \\\n",
    "    .option(\"header\", \"false\") \\\n",
    "    .option(\"sep\", \",\") \\\n",
    "    .load(\"/FileStore/Data/riders.csv\")\n",
    "\n",
    "\n",
    "#Renamed the column name \n",
    "riders_df = riders_df.withColumnRenamed('_c0','rider_id')\n",
    "riders_df = riders_df.withColumnRenamed('_c1','first')\n",
    "riders_df = riders_df.withColumnRenamed('_c2','last')\n",
    "riders_df = riders_df.withColumnRenamed('_c3','address')\n",
    "riders_df = riders_df.withColumnRenamed('_c4','birthday')\n",
    "riders_df = riders_df.withColumnRenamed('_c5','account_start_date')\n",
    "riders_df = riders_df.withColumnRenamed('_c6','account_end_date')\n",
    "riders_df = riders_df.withColumnRenamed('_c7','is_member')\n",
    "\n",
    "\n",
    "riders_df.write.format(\"delta\") \\\n",
    "    .option(\"delta.columnMapping.mode\", \"name\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"mergeSchema\", \"true\") \\\n",
    "    .save(\"/delta/bronze/bronze_riders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fcc84d95-c611-4a39-8a3c-2471120cb589",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+---------+--------------------+----------+------------------+----------------+---------+\n|rider_id|      first|     last|             address|  birthday|account_start_date|account_end_date|is_member|\n+--------+-----------+---------+--------------------+----------+------------------+----------------+---------+\n|    1000|      Diana|    Clark| 1200 Alyssa Squares|1989-02-13|        2019-04-23|            null|     True|\n|    1001|   Jennifer|    Smith|     397 Diana Ferry|1976-08-10|        2019-11-01|      2020-09-01|     True|\n|    1002|      Karen|    Smith|644 Brittany Row ...|1998-08-10|        2022-02-04|            null|     True|\n|    1003|      Bryan|  Roberts|996 Dickerson Tur...|1999-03-29|        2019-08-26|            null|    False|\n|    1004|      Jesse|Middleton|7009 Nathan Expre...|1969-04-11|        2019-09-14|            null|     True|\n|    1005|  Christine|Rodriguez|224 Washington Mi...|1974-08-27|        2020-03-24|            null|    False|\n|    1006|     Alicia|   Taylor|   1137 Angela Locks|2004-01-30|        2020-11-27|      2021-12-01|     True|\n|    1007|   Benjamin|Fernandez|   979 Phillips Ways|1988-01-11|        2016-12-11|            null|    False|\n|    1008|       John| Crawford|    7691 Evans Court|1987-02-21|        2021-03-28|      2021-07-01|     True|\n|    1009|   Victoria|   Ritter|9922 Jim Crest Ap...|1981-02-07|        2020-06-12|      2021-11-01|     True|\n|    1010|      Tracy|   Austin|    92973 Mary Ville|1996-04-07|        2019-12-27|            null|     True|\n|    1011|    Jessica|    Mcgee|950 Grimes Burg A...|1984-12-29|        2017-05-20|            null|     True|\n|    1012|    Heather|   Fisher|65532 Davis Sprin...|1980-10-20|        2021-10-16|            null|     True|\n|    1013|    Timothy|    Jones| 7757 Johnston Roads|1985-07-10|        2020-12-28|      2021-11-01|     True|\n|    1014|   Jennifer|   Martin|   501 Arellano Land|1989-12-04|        2017-11-24|            null|     True|\n|    1015|Christopher|    Silva|3710 Rodriguez Gl...|2001-07-25|        2017-07-10|            null|     True|\n|    1016|     Andrew|    Jones|  72226 Casey Square|1991-12-13|        2022-02-02|            null|     True|\n|    1017|    William|   Lawson| 40395 Terrell Parks|1981-04-17|        2019-03-11|            null|     True|\n|    1018|     Shelly|   Briggs|   3514 Leslie Vista|1986-09-02|        2021-07-25|            null|    False|\n|    1019|       Tina|   Garcia|00348 Brandi Park...|1997-05-03|        2021-07-10|      2022-01-01|    False|\n+--------+-----------+---------+--------------------+----------+------------------+----------------+---------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "riders_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6fbccf90-b660-416f-95cc-92aa3a79dd15",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 3. Stations Tables\n",
    "stations_df = spark.read.format(\"csv\") \\\n",
    "    .option(\"inferSchema\", \"false\") \\\n",
    "    .option(\"header\", \"false\") \\\n",
    "    .option(\"sep\", \",\") \\\n",
    "    .load(\"/FileStore/Data/stations.csv\")\n",
    "\n",
    "\n",
    "#Renamed the column name \n",
    "stations_df = stations_df.withColumnRenamed('_c0','station_id')\n",
    "stations_df = stations_df.withColumnRenamed('_c1','name')\n",
    "stations_df = stations_df.withColumnRenamed('_c2','latitude')\n",
    "stations_df = stations_df.withColumnRenamed('_c3','longtitude')\n",
    "\n",
    "\n",
    "stations_df.write.format(\"delta\") \\\n",
    "    .option(\"delta.columnMapping.mode\", \"name\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"mergeSchema\", \"true\") \\\n",
    "    .save(\"/delta/bronze/bronze_stations\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e41665d-be6d-46b1-8e73-48f485be777d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+------------------+------------------+\n|  station_id|                name|          latitude|        longtitude|\n+------------+--------------------+------------------+------------------+\n|         525|Glenwood Ave & To...|         42.012701|-87.66605799999999|\n|KA1503000012|  Clark St & Lake St| 41.88579466666667|-87.63110066666668|\n|         637|Wood St & Chicago...|         41.895634|        -87.672069|\n|       13216|  State St & 33rd St|        41.8347335|       -87.6258275|\n|       18003|Fairbanks St & Su...| 41.89580766666667|-87.62025316666669|\n|KP1705001026|LaSalle Dr & Huro...|         41.894877|        -87.632326|\n|       13253|Lincoln Ave & Wav...|         41.948797|        -87.675278|\n|KA1503000044|Rush St & Hubbard St|         41.890173|-87.62618499999999|\n|KA1504000140|Winchester Ave & ...| 41.92403733333333|-87.67641483333334|\n|TA1305000032|Clinton St & Madi...|         41.882242|-87.64106600000001|\n|TA1306000012| Wells St & Huron St| 41.89475366666667|-87.63440200000001|\n|       13133|Damen Ave & Cortl...|41.915983000000004|        -87.677335|\n|      SL-005|Indiana Ave & Roo...|         41.867888|        -87.623041|\n|       13235|Southport Ave & W...|          41.94815|         -87.66394|\n|TA1307000139| MLK Jr Dr & 29th St|         41.842052|           -87.617|\n|TA1305000009|Clark St & Ida B ...|     41.8759326655|-87.63058453549999|\n|       13276|Stockton Dr & Wri...|        41.9313455|-87.63869133333333|\n|TA1307000107|Sheridan Rd & Mon...|          41.96167|         -87.65464|\n|       13193|Larrabee St & Web...|         41.921822|-87.64414000000001|\n|KA1503000072|Wacker Dr & Washi...|         41.883132|        -87.637321|\n+------------+--------------------+------------------+------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "stations_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20ecd4a9-659e-4fec-b3e3-d623304686e5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 4. Trips Tables\n",
    "trips_df = spark.read.format(\"csv\") \\\n",
    "    .option(\"inferSchema\", \"false\") \\\n",
    "    .option(\"header\", \"false\") \\\n",
    "    .option(\"sep\", \",\") \\\n",
    "    .load(\"/FileStore/Data/trips.csv\")\n",
    "\n",
    "\n",
    "#Renamed the column name \n",
    "trips_df = trips_df.withColumnRenamed('_c0','trip_id')\n",
    "trips_df = trips_df.withColumnRenamed('_c1','rideable_type')\n",
    "trips_df = trips_df.withColumnRenamed('_c2','start_at')\n",
    "trips_df = trips_df.withColumnRenamed('_c3','ended_at')  \n",
    "trips_df = trips_df.withColumnRenamed('_c4','start_station_id')  \n",
    "trips_df = trips_df.withColumnRenamed('_c5','end_station_id')  \n",
    "trips_df = trips_df.withColumnRenamed('_c6','rider_id')  \n",
    "\n",
    "\n",
    "trips_df.write.format(\"delta\") \\\n",
    "    .option(\"delta.columnMapping.mode\", \"name\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"mergeSchema\", \"true\") \\\n",
    "    .save(\"/delta/bronze/bronze_trips\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "834383f0-cf36-47b1-99c3-d226915ac5ef",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------------+-------------------+-------------------+----------------+--------------+--------+\n|         trip_id|rideable_type|           start_at|           ended_at|start_station_id|end_station_id|rider_id|\n+----------------+-------------+-------------------+-------------------+----------------+--------------+--------+\n|89E7AA6C29227EFF| classic_bike|2021-02-12 16:14:56|2021-02-12 16:21:43|             525|           660|   71934|\n|0FEFDE2603568365| classic_bike|2021-02-14 17:52:38|2021-02-14 18:12:09|             525|         16806|   47854|\n|E6159D746B2DBB91|electric_bike|2021-02-09 19:10:18|2021-02-09 19:19:10|    KA1503000012|  TA1305000029|   70870|\n|B32D3199F1C2E75B| classic_bike|2021-02-02 17:49:41|2021-02-02 17:54:06|             637|  TA1305000034|   58974|\n|83E463F23575F4BF|electric_bike|2021-02-23 15:07:23|2021-02-23 15:22:37|           13216|  TA1309000055|   39608|\n|BDAA7E3494E8D545|electric_bike|2021-02-24 15:43:33|2021-02-24 15:49:05|           18003|  KP1705001026|   36267|\n|A772742351171257| classic_bike|2021-02-01 17:47:42|2021-02-01 17:48:33|    KP1705001026|  KP1705001026|   50104|\n|295476889D9B79F8| classic_bike|2021-02-11 18:33:53|2021-02-11 18:35:09|           18003|         18003|   19618|\n|362087194BA4CC9A| classic_bike|2021-02-27 15:13:39|2021-02-27 15:36:36|    KP1705001026|  KP1705001026|   16732|\n|21630F715038CCB0| classic_bike|2021-02-20 08:59:42|2021-02-20 09:17:04|    KP1705001026|  KP1705001026|   57068|\n|A977EB7FE7F5CD3A| classic_bike|2021-02-20 08:58:16|2021-02-20 08:58:41|    KP1705001026|  KP1705001026|   32712|\n|8B868B03D6753C2A| classic_bike|2021-02-20 16:45:11|2021-02-20 16:59:47|    KP1705001026|  KP1705001026|   23227|\n|BD331D658B9D2C31| classic_bike|2021-02-18 13:21:03|2021-02-18 13:25:20|             525|           520|   73221|\n|8DFEA9BAFE6BAA62| classic_bike|2021-02-26 17:40:05|2021-02-26 17:42:49|           13253|  TA1309000050|   22163|\n|27BE9F6E67AFD86C| classic_bike|2021-02-06 14:40:25|2021-02-06 14:55:50|             525|         15578|    7566|\n|9B790D47A0A0F7F1| classic_bike|2021-02-19 23:25:40|2021-02-20 00:10:00|    KA1503000044|  KA1504000142|   71588|\n|3C2DF72600B1DE6C| classic_bike|2021-02-18 23:20:10|2021-02-19 00:01:39|    KA1503000044|  KA1504000142|   38661|\n|48A8D07ED9C7065C| classic_bike|2021-02-20 23:35:29|2021-02-21 00:17:18|    KA1503000044|  KA1504000142|   64751|\n|BBFF2AAA0A3A1A26|electric_bike|2021-02-02 15:48:52|2021-02-02 16:03:40|    KA1504000140|         17660|   10721|\n|030723CBA8CF05E7| classic_bike|2021-02-23 07:44:12|2021-02-23 07:48:57|    TA1305000032|         15542|   13281|\n+----------------+-------------+-------------------+-------------------+----------------+--------------+--------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "trips_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08a5a067-4607-4621-9c13-209037c0aa57",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Step 2: Create a Gold Data Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "759a4bf2-2e62-419d-bce4-c7055e9ccee5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+------+--------+\n|payment_id|      date|amount|rider_id|\n+----------+----------+------+--------+\n|         1|2019-05-01|  9.00|    1000|\n|         2|2019-06-01|  9.00|    1000|\n|         3|2019-07-01|  9.00|    1000|\n|         4|2019-08-01|  9.00|    1000|\n|         5|2019-09-01|  9.00|    1000|\n|         6|2019-10-01|  9.00|    1000|\n|         7|2019-11-01|  9.00|    1000|\n|         8|2019-12-01|  9.00|    1000|\n|         9|2020-01-01|  9.00|    1000|\n|        10|2020-02-01|  9.00|    1000|\n|        11|2020-03-01|  9.00|    1000|\n|        12|2020-04-01|  9.00|    1000|\n|        13|2020-05-01|  9.00|    1000|\n|        14|2020-06-01|  9.00|    1000|\n|        15|2020-07-01|  9.00|    1000|\n|        16|2020-08-01|  9.00|    1000|\n|        17|2020-09-01|  9.00|    1000|\n|        18|2020-10-01|  9.00|    1000|\n|        19|2020-11-01|  9.00|    1000|\n|        20|2020-12-01|  9.00|    1000|\n+----------+----------+------+--------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "payment_bronze_df = spark.read.format(\"delta\").load(\"/delta/bronze/bronze_payments\")\n",
    "payment_bronze_df.createOrReplaceTempView(\"payment_bronze_view\")\n",
    "\n",
    "payment_gold_df = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        payment_id,\n",
    "        TO_DATE(date) AS date,\n",
    "        CAST(amount AS DECIMAL(10, 2)) AS amount,\n",
    "        rider_id\n",
    "    FROM\n",
    "        payment_bronze_view\n",
    "    WHERE\n",
    "        date IS NOT NULL\n",
    "\"\"\")\n",
    "\n",
    "payment_gold_df.write.format(\"delta\").mode(\"overwrite\").save(\"/delta/gold/payment\")\n",
    "payment_gold_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9ecc6a2f-394e-4b51-8585-9c64852df4dc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "payment_gold_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73cfcba9-3f64-4709-88f8-1d1bfd3b7527",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Create Rider table for gold store\n",
    "riders_bronze_df = spark.read.format(\"delta\").load(\"/delta/bronze/bronze_riders\")\n",
    "riders_bronze_df.createOrReplaceTempView(\"riders_bronze_view\")\n",
    "\n",
    "riders_gold_df = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        riders_bronze_view.rider_id,\n",
    "        first,\n",
    "        last,\n",
    "        address,\n",
    "        TO_DATE(birthday) AS birthday,\n",
    "        TO_DATE(account_start_date) AS account_start_date,\n",
    "        TO_DATE(account_end_date) AS account_end_date,\n",
    "        is_member\n",
    "    FROM\n",
    "        riders_bronze_view\n",
    "    WHERE\n",
    "        rider_id IS NOT NULL\n",
    "\"\"\")\n",
    "\n",
    "# Store 'riders' DataFrame as Delta Lake table\n",
    "riders_gold_df.write.format(\"delta\").mode(\"overwrite\").save(\"/delta/gold/riders\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5379531-c16a-46f0-b6f8-3ea0052de9a4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+---------+--------------------+----------+------------------+----------------+---------+\n|rider_id|      first|     last|             address|  birthday|account_start_date|account_end_date|is_member|\n+--------+-----------+---------+--------------------+----------+------------------+----------------+---------+\n|    1000|      Diana|    Clark| 1200 Alyssa Squares|1989-02-13|        2019-04-23|            null|     True|\n|    1001|   Jennifer|    Smith|     397 Diana Ferry|1976-08-10|        2019-11-01|      2020-09-01|     True|\n|    1002|      Karen|    Smith|644 Brittany Row ...|1998-08-10|        2022-02-04|            null|     True|\n|    1003|      Bryan|  Roberts|996 Dickerson Tur...|1999-03-29|        2019-08-26|            null|    False|\n|    1004|      Jesse|Middleton|7009 Nathan Expre...|1969-04-11|        2019-09-14|            null|     True|\n|    1005|  Christine|Rodriguez|224 Washington Mi...|1974-08-27|        2020-03-24|            null|    False|\n|    1006|     Alicia|   Taylor|   1137 Angela Locks|2004-01-30|        2020-11-27|      2021-12-01|     True|\n|    1007|   Benjamin|Fernandez|   979 Phillips Ways|1988-01-11|        2016-12-11|            null|    False|\n|    1008|       John| Crawford|    7691 Evans Court|1987-02-21|        2021-03-28|      2021-07-01|     True|\n|    1009|   Victoria|   Ritter|9922 Jim Crest Ap...|1981-02-07|        2020-06-12|      2021-11-01|     True|\n|    1010|      Tracy|   Austin|    92973 Mary Ville|1996-04-07|        2019-12-27|            null|     True|\n|    1011|    Jessica|    Mcgee|950 Grimes Burg A...|1984-12-29|        2017-05-20|            null|     True|\n|    1012|    Heather|   Fisher|65532 Davis Sprin...|1980-10-20|        2021-10-16|            null|     True|\n|    1013|    Timothy|    Jones| 7757 Johnston Roads|1985-07-10|        2020-12-28|      2021-11-01|     True|\n|    1014|   Jennifer|   Martin|   501 Arellano Land|1989-12-04|        2017-11-24|            null|     True|\n|    1015|Christopher|    Silva|3710 Rodriguez Gl...|2001-07-25|        2017-07-10|            null|     True|\n|    1016|     Andrew|    Jones|  72226 Casey Square|1991-12-13|        2022-02-02|            null|     True|\n|    1017|    William|   Lawson| 40395 Terrell Parks|1981-04-17|        2019-03-11|            null|     True|\n|    1018|     Shelly|   Briggs|   3514 Leslie Vista|1986-09-02|        2021-07-25|            null|    False|\n|    1019|       Tina|   Garcia|00348 Brandi Park...|1997-05-03|        2021-07-10|      2022-01-01|    False|\n+--------+-----------+---------+--------------------+----------+------------------+----------------+---------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "riders_gold_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0891652-7f2f-4ce5-bfca-ad421e6fb98d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Create Station table for gold store\n",
    "stations_bronze_df = spark.read.format(\"delta\").load(\"/delta/bronze/bronze_stations\")\n",
    "stations_bronze_df.createOrReplaceTempView(\"stations_bronze_view\")\n",
    "\n",
    "stations_gold_df = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        stations_bronze_view.station_id,\n",
    "        name,\n",
    "        CAST(latitude AS FLOAT) AS latitude,\n",
    "        CAST(longtitude AS FLOAT) AS longitude\n",
    "    FROM\n",
    "        stations_bronze_view\n",
    "    WHERE\n",
    "        station_id IS NOT NULL\n",
    "\"\"\")\n",
    "\n",
    "# Store 'Stations' DataFrame as Delta Lake table\n",
    "stations_gold_df.write.format(\"delta\").mode(\"overwrite\").save(\"/delta/gold/stations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4cfbb6f2-0979-4cef-aab5-4c154efe2c28",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+---------+----------+\n|  station_id|                name| latitude| longitude|\n+------------+--------------------+---------+----------+\n|         525|Glenwood Ave & To...|  42.0127| -87.66606|\n|KA1503000012|  Clark St & Lake St|41.885796|  -87.6311|\n|         637|Wood St & Chicago...|41.895634|-87.672066|\n|       13216|  State St & 33rd St|41.834732|-87.625824|\n|       18003|Fairbanks St & Su...| 41.89581|-87.620255|\n|KP1705001026|LaSalle Dr & Huro...| 41.89488|-87.632324|\n|       13253|Lincoln Ave & Wav...|41.948795| -87.67528|\n|KA1503000044|Rush St & Hubbard St|41.890175| -87.62618|\n|KA1504000140|Winchester Ave & ...|41.924038|-87.676414|\n|TA1305000032|Clinton St & Madi...| 41.88224| -87.64107|\n|TA1306000012| Wells St & Huron St|41.894753|  -87.6344|\n|       13133|Damen Ave & Cortl...| 41.91598| -87.67734|\n|      SL-005|Indiana Ave & Roo...| 41.86789| -87.62304|\n|       13235|Southport Ave & W...| 41.94815| -87.66394|\n|TA1307000139| MLK Jr Dr & 29th St|41.842052|   -87.617|\n|TA1305000009|Clark St & Ida B ...| 41.87593|-87.630585|\n|       13276|Stockton Dr & Wri...|41.931347|-87.638695|\n|TA1307000107|Sheridan Rd & Mon...| 41.96167| -87.65464|\n|       13193|Larrabee St & Web...| 41.92182| -87.64414|\n|KA1503000072|Wacker Dr & Washi...|41.883133| -87.63732|\n+------------+--------------------+---------+----------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "stations_gold_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8afbb5ae-25f6-4e41-ab70-471dfd9eb123",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Create Trips tables for gold store\n",
    "trips_bronze_df = spark.read.format(\"delta\").load(\"/delta/bronze/bronze_trips\")\n",
    "trips_bronze_df.createOrReplaceTempView(\"trips_bronze_view\")\n",
    "\n",
    "trips_gold_df = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        trips_bronze_view.trip_id,\n",
    "        rideable_type,\n",
    "        start_at,\n",
    "        ended_at,\n",
    "        start_station_id,\n",
    "        end_station_id,\n",
    "        rider_id\n",
    "    FROM\n",
    "        trips_bronze_view\n",
    "    WHERE\n",
    "        trip_id IS NOT NULL\n",
    "\"\"\")\n",
    "\n",
    "# Store 'trips' DataFrame as Delta Lake table\n",
    "trips_gold_df.write.format(\"delta\").mode(\"overwrite\").save(\"/delta/gold/trips\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3cfd6c86-1228-4f74-ab0d-df5e5433acb9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------------+-------------------+-------------------+----------------+--------------+--------+\n|         trip_id|rideable_type|           start_at|           ended_at|start_station_id|end_station_id|rider_id|\n+----------------+-------------+-------------------+-------------------+----------------+--------------+--------+\n|222BB8E5059252D7| classic_bike|2021-06-13 09:48:47|2021-06-13 10:07:23|    KA1503000064|         13021|   34062|\n|1826E16CB5486018| classic_bike|2021-06-21 22:59:13|2021-06-21 23:04:29|    TA1306000010|         13021|    5342|\n|3D9B6A0A5330B04D| classic_bike|2021-06-18 16:06:42|2021-06-18 16:12:02|    TA1305000030|         13021|    3714|\n|07E82F5E9C9E490F| classic_bike|2021-06-17 16:46:23|2021-06-17 17:02:45|    TA1305000034|         13021|   18793|\n|A8E94BAECBF0C2DD|  docked_bike|2021-06-13 17:36:29|2021-06-13 18:30:39|    TA1308000009|  TA1308000009|   43342|\n|378F4AB323AA1D14|  docked_bike|2021-06-13 13:20:10|2021-06-13 14:06:14|    TA1308000009|  TA1308000009|    6693|\n|38AD311DC2EB1FBE|  docked_bike|2021-06-16 17:14:30|2021-06-16 17:28:34|    KA1503000019|  KA1503000019|   71480|\n|1D466737F0B18097|  docked_bike|2021-06-27 14:51:52|2021-06-27 15:26:39|    TA1308000009|  TA1308000009|   50846|\n|27E1142E1ACFAEFB|electric_bike|2021-06-21 13:58:26|2021-06-21 13:58:53|           13257|         13257|   18951|\n|67F2A115DAE77924| classic_bike|2021-06-22 00:51:43|2021-06-22 01:08:25|    TA1308000009|  TA1308000009|   63987|\n|D30A0F588CAB9948|  docked_bike|2021-06-03 16:40:02|2021-06-03 17:04:22|           13036|         13434|   52600|\n|B6D051538810CE8A| classic_bike|2021-06-14 19:19:04|2021-06-14 19:55:54|    KA1503000064|  TA1308000009|   58393|\n|9724F787201C914E| classic_bike|2021-06-06 19:37:03|2021-06-06 19:56:22|           13042|         13434|   45636|\n|5C07EC15B5E1F93C| classic_bike|2021-06-19 20:17:01|2021-06-19 20:27:19|           13084|         13257|   66194|\n|1BFBE159F63DF1AF| classic_bike|2021-06-30 09:06:25|2021-06-30 09:09:11|    KA1503000043|         13021|   18576|\n|C63897EB5765B1E6| classic_bike|2021-06-11 22:46:36|2021-06-11 22:49:03|    TA1306000003|         13021|   62971|\n|AAA3B86E0B459EBB|electric_bike|2021-06-10 15:56:15|2021-06-10 16:03:51|    KA1503000043|         13434|    8235|\n|B22FA59E25D85FFB|electric_bike|2021-06-10 17:20:02|2021-06-10 17:26:05|           13157|         13021|   51431|\n|ADA725E548D6CF84|electric_bike|2021-06-08 16:12:24|2021-06-08 16:19:32|    KA1503000043|         13434|   40507|\n|AC061F7C81D386EE|electric_bike|2021-06-25 10:04:56|2021-06-25 10:15:01|           13016|         13021|   67393|\n+----------------+-------------+-------------------+-------------------+----------------+--------------+--------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "trips_gold_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "beb2e95c-bf77-4100-b9b0-518edced832d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Input the table to databricks trips tables\n",
    "trips_gold_df_table = spark.read.format(\"delta\") \\\n",
    "    .load(\"/delta/gold/trips\")\n",
    "\n",
    "trips_gold_df_table.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(\"trips\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ad37fda-9397-4a15-8ae2-34bd03754316",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[60]: DataFrame[num_affected_rows: bigint, num_inserted_rows: bigint]"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE Trip_Fact\n",
    "    USING delta\n",
    "    AS SELECT * FROM trips\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a488651-63e2-4369-a05f-ed23d5031ed2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Input the table to databricks payments tables\n",
    "payment_gold_df_table = spark.read.format(\"delta\") \\\n",
    "    .load(\"/delta/gold/payment\")\n",
    "\n",
    "payment_gold_df_table.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(\"payment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71237d3c-3d3a-48bd-9a7e-34ab4f78b88a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[6]: DataFrame[num_affected_rows: bigint, num_inserted_rows: bigint]"
     ]
    }
   ],
   "source": [
    "# Create Payment fact table from the payment table\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE Payment_Fact\n",
    "    USING delta\n",
    "    AS SELECT * FROM payment\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6947c3fb-c926-4ccc-8d7c-4b49a3ab58bd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[7]: DataFrame[num_affected_rows: bigint, num_inserted_rows: bigint]"
     ]
    }
   ],
   "source": [
    "# Create Date_Dim\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE Date_Dim\n",
    "    USING delta\n",
    "    AS SELECT DISTINCT date,\n",
    "        day(date) AS day,\n",
    "        dayofweek(date) AS day_of_week,\n",
    "        month(date) AS month,\n",
    "        quarter(date) AS quarter,\n",
    "        year(date) AS year,\n",
    "        CASE WHEN dayofweek(date) IN (1, 7) THEN 1 ELSE 0 END AS is_weekend\n",
    "    FROM payment\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac009df3-15dd-411a-8265-112ba14e2ca4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[9]: DataFrame[num_affected_rows: bigint, num_inserted_rows: bigint]"
     ]
    }
   ],
   "source": [
    "# Create Time_Dim\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE Time_Dim\n",
    "    USING delta\n",
    "    AS\n",
    "    SELECT DISTINCT\n",
    "        start_at AS time,\n",
    "        HOUR(start_at) AS hour,\n",
    "        MINUTE(start_at) AS minute,\n",
    "        SECOND(start_at) AS second\n",
    "    FROM trips\n",
    "    UNION\n",
    "    SELECT DISTINCT\n",
    "        ended_at AS time,\n",
    "        HOUR(ended_at) AS hour,\n",
    "        MINUTE(ended_at) AS minute,\n",
    "        SECOND(ended_at) AS second\n",
    "    FROM trips\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd71e476-f663-461a-b18e-193cdc769a5b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Input the table to databricks stations tables\n",
    "stations_gold_df_table = spark.read.format(\"delta\") \\\n",
    "    .load(\"/delta/gold/stations\")\n",
    "\n",
    "stations_gold_df_table.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(\"stations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2fc5f04-e384-4fbf-a7e6-ad8104094ea6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[11]: DataFrame[num_affected_rows: bigint, num_inserted_rows: bigint]"
     ]
    }
   ],
   "source": [
    "# Create Station_Dim\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE Station_Dim\n",
    "    USING delta\n",
    "    AS SELECT * FROM stations\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d864abfe-b1cc-4d1a-90be-1e18d575ff42",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Input riders to databricks\n",
    "riders_gold_df_table = spark.read.format(\"delta\") \\\n",
    "    .load(\"/delta/gold/riders\")\n",
    "\n",
    "riders_gold_df_table.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(\"riders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0656875b-8907-4ec4-b0d3-ef02b7e57af3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[22]: DataFrame[num_affected_rows: bigint, num_inserted_rows: bigint]"
     ]
    }
   ],
   "source": [
    "# Create Rider_Dim\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE Rider_Dim\n",
    "    USING delta\n",
    "    AS SELECT * FROM riders\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4df69a1a-3681-4d7d-8d24-d2cbcd8332b3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Step 4: Transforming Data into the Star Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2901fac9-acb0-4385-83d8-5db1dc9450bd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[12]: DataFrame[num_affected_rows: bigint, num_inserted_rows: bigint]"
     ]
    }
   ],
   "source": [
    "# Fact Table insert\n",
    "spark.sql(\"\"\"\n",
    "    INSERT INTO Payment_Fact\n",
    "    SELECT *\n",
    "    FROM payment\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6b5e640-96a9-45a5-b61e-ed8ee8a363c8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[13]: DataFrame[num_affected_rows: bigint, num_inserted_rows: bigint]"
     ]
    }
   ],
   "source": [
    "# Date_Dim Insert\n",
    "spark.sql(\"\"\"\n",
    "    INSERT INTO Date_Dim\n",
    "    SELECT DISTINCT date,\n",
    "        day(date) AS day,\n",
    "        dayofweek(date) AS day_of_week,\n",
    "        month(date) AS month,\n",
    "        quarter(date) AS quarter,\n",
    "        year(date) AS year,\n",
    "        CASE WHEN dayofweek(date) IN (1, 7) THEN 1 ELSE 0 END AS is_weekend\n",
    "    FROM payment\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36fe27ff-1747-47f0-b48d-215e0367b4bc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[14]: DataFrame[num_affected_rows: bigint, num_inserted_rows: bigint]"
     ]
    }
   ],
   "source": [
    "# Time_Dim Insert\n",
    "spark.sql(\"\"\"\n",
    "    INSERT INTO Time_Dim\n",
    "    SELECT DISTINCT\n",
    "        start_at AS time,\n",
    "        HOUR(start_at) AS hour,\n",
    "        MINUTE(start_at) AS minute,\n",
    "        SECOND(start_at) AS second\n",
    "    FROM trips\n",
    "    UNION\n",
    "    SELECT DISTINCT\n",
    "        ended_at AS time,\n",
    "        HOUR(ended_at) AS hour,\n",
    "        MINUTE(ended_at) AS minute,\n",
    "        SECOND(ended_at) AS second\n",
    "    FROM trips\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "733ab4f9-4f5c-4a36-bd2a-aab8506b85a4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[15]: DataFrame[num_affected_rows: bigint, num_inserted_rows: bigint]"
     ]
    }
   ],
   "source": [
    "# Station_Dim Insert\n",
    "spark.sql(\"\"\"\n",
    "    INSERT INTO Station_Dim\n",
    "    SELECT *\n",
    "    FROM stations\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a1308ae-46d9-4368-a188-4674b096756b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[23]: DataFrame[num_affected_rows: bigint, num_inserted_rows: bigint]"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    INSERT INTO Rider_Dim\n",
    "    SELECT \n",
    "        rider_id,\n",
    "        first,\n",
    "        last,\n",
    "        address,\n",
    "        birthday,\n",
    "        account_start_date,\n",
    "        account_end_date,\n",
    "        is_member\n",
    "    FROM riders\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e1f568aa-2919-4947-b51b-5fd36100ff20",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Analyze money spent (using Payment_Fact table):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "497c1c36-d18c-4eb1-b8ef-e1ab1fd65a56",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+----+------------+\n|month|quarter|year|total_amount|\n+-----+-------+----+------------+\n|    2|      1|2022|  2397885.08|\n|    1|      1|2022|  2361996.72|\n|   12|      4|2021|  2316919.68|\n|   11|      4|2021|  2264562.44|\n|   10|      4|2021|  2222626.40|\n|    9|      3|2021|  2163974.12|\n|    8|      3|2021|  2109123.04|\n|    7|      3|2021|  2052054.68|\n|    6|      2|2021|  2000826.60|\n|    5|      2|2021|  1942877.00|\n|    4|      2|2021|  1887006.64|\n|    3|      1|2021|  1835136.68|\n|    2|      1|2021|  1789721.48|\n|    1|      1|2021|  1739564.24|\n|   12|      4|2020|  1682818.20|\n|   11|      4|2020|  1639681.76|\n|   10|      4|2020|  1593460.12|\n|    9|      3|2020|  1542305.36|\n|    8|      3|2020|  1497586.20|\n|    7|      3|2020|  1449733.96|\n+-----+-------+----+------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT month, quarter, year, SUM(amount) AS total_amount\n",
    "    FROM Payment_Fact PF, Date_Dim DD\n",
    "    WHERE PF.`date` = DD.`date`\n",
    "    GROUP BY month, quarter, year\n",
    "    ORDER BY total_amount DESC\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ba0fe8d-fc9e-48e6-94f5-4e41b8871ca4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+------+--------+\n|payment_id|      date|amount|rider_id|\n+----------+----------+------+--------+\n|         1|2019-05-01|  9.00|    1000|\n|         2|2019-06-01|  9.00|    1000|\n|         3|2019-07-01|  9.00|    1000|\n|         4|2019-08-01|  9.00|    1000|\n|         5|2019-09-01|  9.00|    1000|\n|         6|2019-10-01|  9.00|    1000|\n|         7|2019-11-01|  9.00|    1000|\n|         8|2019-12-01|  9.00|    1000|\n|         9|2020-01-01|  9.00|    1000|\n|        10|2020-02-01|  9.00|    1000|\n|        11|2020-03-01|  9.00|    1000|\n|        12|2020-04-01|  9.00|    1000|\n|        13|2020-05-01|  9.00|    1000|\n|        14|2020-06-01|  9.00|    1000|\n|        15|2020-07-01|  9.00|    1000|\n|        16|2020-08-01|  9.00|    1000|\n|        17|2020-09-01|  9.00|    1000|\n|        18|2020-10-01|  9.00|    1000|\n|        19|2020-11-01|  9.00|    1000|\n|        20|2020-12-01|  9.00|    1000|\n+----------+----------+------+--------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT *\n",
    "    FROM Payment_Fact PF\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e086895b-77dd-43ec-8b21-8650a70e1193",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+---------+--------------------+----------+------------------+----------------+---------+\n|rider_id|      first|     last|             address|  birthday|account_start_date|account_end_date|is_member|\n+--------+-----------+---------+--------------------+----------+------------------+----------------+---------+\n|    1000|      Diana|    Clark| 1200 Alyssa Squares|1989-02-13|        2019-04-23|            null|     True|\n|    1001|   Jennifer|    Smith|     397 Diana Ferry|1976-08-10|        2019-11-01|      2020-09-01|     True|\n|    1002|      Karen|    Smith|644 Brittany Row ...|1998-08-10|        2022-02-04|            null|     True|\n|    1003|      Bryan|  Roberts|996 Dickerson Tur...|1999-03-29|        2019-08-26|            null|    False|\n|    1004|      Jesse|Middleton|7009 Nathan Expre...|1969-04-11|        2019-09-14|            null|     True|\n|    1005|  Christine|Rodriguez|224 Washington Mi...|1974-08-27|        2020-03-24|            null|    False|\n|    1006|     Alicia|   Taylor|   1137 Angela Locks|2004-01-30|        2020-11-27|      2021-12-01|     True|\n|    1007|   Benjamin|Fernandez|   979 Phillips Ways|1988-01-11|        2016-12-11|            null|    False|\n|    1008|       John| Crawford|    7691 Evans Court|1987-02-21|        2021-03-28|      2021-07-01|     True|\n|    1009|   Victoria|   Ritter|9922 Jim Crest Ap...|1981-02-07|        2020-06-12|      2021-11-01|     True|\n|    1010|      Tracy|   Austin|    92973 Mary Ville|1996-04-07|        2019-12-27|            null|     True|\n|    1011|    Jessica|    Mcgee|950 Grimes Burg A...|1984-12-29|        2017-05-20|            null|     True|\n|    1012|    Heather|   Fisher|65532 Davis Sprin...|1980-10-20|        2021-10-16|            null|     True|\n|    1013|    Timothy|    Jones| 7757 Johnston Roads|1985-07-10|        2020-12-28|      2021-11-01|     True|\n|    1014|   Jennifer|   Martin|   501 Arellano Land|1989-12-04|        2017-11-24|            null|     True|\n|    1015|Christopher|    Silva|3710 Rodriguez Gl...|2001-07-25|        2017-07-10|            null|     True|\n|    1016|     Andrew|    Jones|  72226 Casey Square|1991-12-13|        2022-02-02|            null|     True|\n|    1017|    William|   Lawson| 40395 Terrell Parks|1981-04-17|        2019-03-11|            null|     True|\n|    1018|     Shelly|   Briggs|   3514 Leslie Vista|1986-09-02|        2021-07-25|            null|    False|\n|    1019|       Tina|   Garcia|00348 Brandi Park...|1997-05-03|        2021-07-10|      2022-01-01|    False|\n+--------+-----------+---------+--------------------+----------+------------------+----------------+---------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT *\n",
    "    FROM rider_dim PF\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48682e7f-b178-451d-ad17-ac49c1ad05e4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+---------+----------+\n|  station_id|                name| latitude| longitude|\n+------------+--------------------+---------+----------+\n|         525|Glenwood Ave & To...|  42.0127| -87.66606|\n|KA1503000012|  Clark St & Lake St|41.885796|  -87.6311|\n|         637|Wood St & Chicago...|41.895634|-87.672066|\n|       13216|  State St & 33rd St|41.834732|-87.625824|\n|       18003|Fairbanks St & Su...| 41.89581|-87.620255|\n|KP1705001026|LaSalle Dr & Huro...| 41.89488|-87.632324|\n|       13253|Lincoln Ave & Wav...|41.948795| -87.67528|\n|KA1503000044|Rush St & Hubbard St|41.890175| -87.62618|\n|KA1504000140|Winchester Ave & ...|41.924038|-87.676414|\n|TA1305000032|Clinton St & Madi...| 41.88224| -87.64107|\n|TA1306000012| Wells St & Huron St|41.894753|  -87.6344|\n|       13133|Damen Ave & Cortl...| 41.91598| -87.67734|\n|      SL-005|Indiana Ave & Roo...| 41.86789| -87.62304|\n|       13235|Southport Ave & W...| 41.94815| -87.66394|\n|TA1307000139| MLK Jr Dr & 29th St|41.842052|   -87.617|\n|TA1305000009|Clark St & Ida B ...| 41.87593|-87.630585|\n|       13276|Stockton Dr & Wri...|41.931347|-87.638695|\n|TA1307000107|Sheridan Rd & Mon...| 41.96167| -87.65464|\n|       13193|Larrabee St & Web...| 41.92182| -87.64414|\n|KA1503000072|Wacker Dr & Washi...|41.883133| -87.63732|\n+------------+--------------------+---------+----------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT *\n",
    "    FROM station_dim\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e02007c4-4785-45fb-93af-a502aec5d26c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+-----------+-----+-------+----+----------+\n|      date|day|day_of_week|month|quarter|year|is_weekend|\n+----------+---+-----------+-----+-------+----+----------+\n|2017-02-01|  1|          4|    2|      1|2017|         0|\n|2018-07-01|  1|          1|    7|      3|2018|         1|\n|2021-08-01|  1|          1|    8|      3|2021|         1|\n|2021-11-01|  1|          2|   11|      4|2021|         0|\n|2017-08-01|  1|          3|    8|      3|2017|         0|\n|2021-05-01|  1|          7|    5|      2|2021|         1|\n|2020-01-01|  1|          4|    1|      1|2020|         0|\n|2019-02-01|  1|          6|    2|      1|2019|         0|\n|2015-07-01|  1|          4|    7|      3|2015|         0|\n|2015-03-01|  1|          1|    3|      1|2015|         1|\n|2013-08-01|  1|          5|    8|      3|2013|         0|\n|2021-03-01|  1|          2|    3|      1|2021|         0|\n|2015-04-01|  1|          4|    4|      2|2015|         0|\n|2020-05-01|  1|          6|    5|      2|2020|         0|\n|2014-01-01|  1|          4|    1|      1|2014|         0|\n|2020-09-01|  1|          3|    9|      3|2020|         0|\n|2015-09-01|  1|          3|    9|      3|2015|         0|\n|2019-12-01|  1|          1|   12|      4|2019|         1|\n|2016-01-01|  1|          6|    1|      1|2016|         0|\n|2020-06-01|  1|          2|    6|      2|2020|         0|\n+----------+---+-----------+-----+-------+----+----------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT *\n",
    "    FROM date_dim\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6926eff2-6a9c-46bb-a115-4d07c2fde1dd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----+------+------+\n|               time|hour|minute|second|\n+-------------------+----+------+------+\n|2021-06-29 15:44:47|  15|    44|    47|\n|2021-06-10 15:32:58|  15|    32|    58|\n|2021-06-11 17:32:34|  17|    32|    34|\n|2021-06-24 19:28:59|  19|    28|    59|\n|2021-06-18 19:03:56|  19|     3|    56|\n|2021-06-12 16:12:36|  16|    12|    36|\n|2021-06-27 21:22:15|  21|    22|    15|\n|2021-06-03 13:22:56|  13|    22|    56|\n|2021-06-14 16:59:02|  16|    59|     2|\n|2021-06-19 22:43:09|  22|    43|     9|\n|2021-06-09 19:36:49|  19|    36|    49|\n|2021-06-08 10:06:26|  10|     6|    26|\n|2021-06-21 21:37:54|  21|    37|    54|\n|2021-06-04 18:51:57|  18|    51|    57|\n|2021-06-18 18:22:36|  18|    22|    36|\n|2021-06-11 23:33:48|  23|    33|    48|\n|2021-06-27 14:27:22|  14|    27|    22|\n|2021-06-22 16:17:39|  16|    17|    39|\n|2021-06-26 09:18:41|   9|    18|    41|\n|2021-06-04 02:57:12|   2|    57|    12|\n+-------------------+----+------+------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT *\n",
    "    FROM time_dim\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "897ee933-0442-45d6-8c1a-c9091810f709",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------------+-------------------+-------------------+----------------+--------------+--------+\n|         trip_id|rideable_type|           start_at|           ended_at|start_station_id|end_station_id|rider_id|\n+----------------+-------------+-------------------+-------------------+----------------+--------------+--------+\n|89E7AA6C29227EFF| classic_bike|2021-02-12 16:14:56|2021-02-12 16:21:43|             525|           660|   71934|\n|0FEFDE2603568365| classic_bike|2021-02-14 17:52:38|2021-02-14 18:12:09|             525|         16806|   47854|\n|E6159D746B2DBB91|electric_bike|2021-02-09 19:10:18|2021-02-09 19:19:10|    KA1503000012|  TA1305000029|   70870|\n|B32D3199F1C2E75B| classic_bike|2021-02-02 17:49:41|2021-02-02 17:54:06|             637|  TA1305000034|   58974|\n|83E463F23575F4BF|electric_bike|2021-02-23 15:07:23|2021-02-23 15:22:37|           13216|  TA1309000055|   39608|\n|BDAA7E3494E8D545|electric_bike|2021-02-24 15:43:33|2021-02-24 15:49:05|           18003|  KP1705001026|   36267|\n|A772742351171257| classic_bike|2021-02-01 17:47:42|2021-02-01 17:48:33|    KP1705001026|  KP1705001026|   50104|\n|295476889D9B79F8| classic_bike|2021-02-11 18:33:53|2021-02-11 18:35:09|           18003|         18003|   19618|\n|362087194BA4CC9A| classic_bike|2021-02-27 15:13:39|2021-02-27 15:36:36|    KP1705001026|  KP1705001026|   16732|\n|21630F715038CCB0| classic_bike|2021-02-20 08:59:42|2021-02-20 09:17:04|    KP1705001026|  KP1705001026|   57068|\n|A977EB7FE7F5CD3A| classic_bike|2021-02-20 08:58:16|2021-02-20 08:58:41|    KP1705001026|  KP1705001026|   32712|\n|8B868B03D6753C2A| classic_bike|2021-02-20 16:45:11|2021-02-20 16:59:47|    KP1705001026|  KP1705001026|   23227|\n|BD331D658B9D2C31| classic_bike|2021-02-18 13:21:03|2021-02-18 13:25:20|             525|           520|   73221|\n|8DFEA9BAFE6BAA62| classic_bike|2021-02-26 17:40:05|2021-02-26 17:42:49|           13253|  TA1309000050|   22163|\n|27BE9F6E67AFD86C| classic_bike|2021-02-06 14:40:25|2021-02-06 14:55:50|             525|         15578|    7566|\n|9B790D47A0A0F7F1| classic_bike|2021-02-19 23:25:40|2021-02-20 00:10:00|    KA1503000044|  KA1504000142|   71588|\n|3C2DF72600B1DE6C| classic_bike|2021-02-18 23:20:10|2021-02-19 00:01:39|    KA1503000044|  KA1504000142|   38661|\n|48A8D07ED9C7065C| classic_bike|2021-02-20 23:35:29|2021-02-21 00:17:18|    KA1503000044|  KA1504000142|   64751|\n|BBFF2AAA0A3A1A26|electric_bike|2021-02-02 15:48:52|2021-02-02 16:03:40|    KA1504000140|         17660|   10721|\n|030723CBA8CF05E7| classic_bike|2021-02-23 07:44:12|2021-02-23 07:48:57|    TA1305000032|         15542|   13281|\n+----------------+-------------+-------------------+-------------------+----------------+--------------+--------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT *\n",
    "    FROM trip_fact\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "76c6e079-4154-4f9e-81c3-27ba21ff48fd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Analyze how much money is spent per member (Extra Credit):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ad08f43-442d-4821-b3b7-806d34834f0b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------+-------------------+\n|rider_id|        rider_name|total_amount_earned|\n+--------+------------------+-------------------+\n|    1268|         Kim Lyons|            1692.00|\n|    1316|     Philip Barnes|             108.00|\n|    2141|        Lisa Green|             994.96|\n|    2167|     Sarah Stewart|             540.00|\n|    2222|   Mathew Matthews|            1080.00|\n|    2316|       Sarah Banks|            1800.00|\n|    2361|    Janet Mcdonald|              72.00|\n|    2416|       Rachel Cruz|             108.00|\n|    2575|       Sally Scott|              36.00|\n|    2700|    Randy Chambers|            3276.00|\n|    3008|          Joy King|            1008.00|\n|    3037|      Sophia Smith|            2808.00|\n|    3088|      Maria Martin|             790.24|\n|    3311|      Regina Bowen|             540.00|\n|    3535|  Elizabeth Graham|            1404.00|\n|    3700|       Karen Pitts|              98.48|\n|    3836|    Stephanie Hull|              36.00|\n|    4217|    Jasmine Barnes|             396.00|\n|    4408|      Justin Smith|             108.00|\n|    4521|Melissa Richardson|             360.00|\n+--------+------------------+-------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT rd.rider_id,\n",
    "        CONCAT(rd.first, ' ', rd.last) AS rider_name,\n",
    "        SUM(pf.amount) AS total_amount_earned\n",
    "    FROM Payment_Fact pf\n",
    "    JOIN Rider_Dim rd ON pf.rider_id = rd.rider_id\n",
    "    GROUP BY rd.rider_id, rider_name\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "da5c1c89-a42a-48c6-9095-1127cea11d02",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Step 5 Bussiness outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c09d5149-acd9-4b6a-8d3b-9dc8274b797f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------------------+-------------------+---------------------+\n|         trip_id|         start_time|           end_time|ride_duration_minutes|\n+----------------+-------------------+-------------------+---------------------+\n|89E7AA6C29227EFF|2021-02-12 16:14:56|2021-02-12 16:21:43|                6.783|\n|0FEFDE2603568365|2021-02-14 17:52:38|2021-02-14 18:12:09|               19.517|\n|E6159D746B2DBB91|2021-02-09 19:10:18|2021-02-09 19:19:10|                8.867|\n|B32D3199F1C2E75B|2021-02-02 17:49:41|2021-02-02 17:54:06|                4.417|\n|83E463F23575F4BF|2021-02-23 15:07:23|2021-02-23 15:22:37|               15.233|\n|BDAA7E3494E8D545|2021-02-24 15:43:33|2021-02-24 15:49:05|                5.533|\n|A772742351171257|2021-02-01 17:47:42|2021-02-01 17:48:33|                 0.85|\n|295476889D9B79F8|2021-02-11 18:33:53|2021-02-11 18:35:09|                1.267|\n|362087194BA4CC9A|2021-02-27 15:13:39|2021-02-27 15:36:36|                22.95|\n|21630F715038CCB0|2021-02-20 08:59:42|2021-02-20 09:17:04|               17.367|\n|A977EB7FE7F5CD3A|2021-02-20 08:58:16|2021-02-20 08:58:41|                0.417|\n|8B868B03D6753C2A|2021-02-20 16:45:11|2021-02-20 16:59:47|                 14.6|\n|BD331D658B9D2C31|2021-02-18 13:21:03|2021-02-18 13:25:20|                4.283|\n|8DFEA9BAFE6BAA62|2021-02-26 17:40:05|2021-02-26 17:42:49|                2.733|\n|27BE9F6E67AFD86C|2021-02-06 14:40:25|2021-02-06 14:55:50|               15.417|\n|9B790D47A0A0F7F1|2021-02-19 23:25:40|2021-02-20 00:10:00|               44.333|\n|3C2DF72600B1DE6C|2021-02-18 23:20:10|2021-02-19 00:01:39|               41.483|\n|48A8D07ED9C7065C|2021-02-20 23:35:29|2021-02-21 00:17:18|               41.817|\n|BBFF2AAA0A3A1A26|2021-02-02 15:48:52|2021-02-02 16:03:40|                 14.8|\n|030723CBA8CF05E7|2021-02-23 07:44:12|2021-02-23 07:48:57|                 4.75|\n+----------------+-------------------+-------------------+---------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Analyze how much time is spent per ride:\n",
    "spark.sql(\"\"\"\n",
    "    SELECT trip_id, \n",
    "           TIMESTAMP(start_at) AS start_time, \n",
    "           TIMESTAMP(ended_at) AS end_time, \n",
    "           round((UNIX_TIMESTAMP(ended_at) - UNIX_TIMESTAMP(start_at)) / 60, 3) AS ride_duration_minutes\n",
    "    FROM trip_fact\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f90e81f-f6b9-4c5c-a1c0-d1f2a9b6f8d8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----------+----+------+\n|         trip_id|day_of_week|hour|minute|\n+----------------+-----------+----+------+\n|42D4B6A9BBE42B39|          3|  15|    56|\n|42D4B6A9BBE42B39|          3|  15|    56|\n|246E53CB58DD424D|          1|  21|    26|\n|246E53CB58DD424D|          1|  21|    26|\n|D9FBB3DC6536583C|          1|  12|    32|\n|D9FBB3DC6536583C|          1|  12|    32|\n|3F4708BA32882D58|          3|  20|    59|\n|3F4708BA32882D58|          3|  20|    59|\n|D530A96BED14F1D2|          6|  11|    29|\n|D530A96BED14F1D2|          6|  11|    29|\n|2A03CCF2C1964880|          6|  16|     8|\n|2A03CCF2C1964880|          6|  16|     8|\n|39F7A0625024FEA8|          5|  22|     3|\n|39F7A0625024FEA8|          5|  22|     3|\n|32A4D4A8BC639CDB|          5|  11|    42|\n|32A4D4A8BC639CDB|          5|  11|    42|\n|07E326B5534970C1|          4|  18|    32|\n|07E326B5534970C1|          4|  18|    32|\n|A473AF58FB4060C5|          4|  18|    32|\n|A473AF58FB4060C5|          4|  18|    32|\n+----------------+-----------+----+------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Analyze time-based factors:\n",
    "spark.sql(\"\"\"\n",
    "    SELECT trip_id, \n",
    "           date_dim.day_of_week, \n",
    "           time_dim.hour, \n",
    "           time_dim.minute\n",
    "    FROM trip_fact\n",
    "    JOIN date_dim ON trip_fact.start_at = date_dim.date\n",
    "    JOIN time_dim ON trip_fact.start_at = time_dim.time\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7127c5e-2f71-4a48-9280-27cda9b95cf0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------------+--------------+\n|         trip_id|start_station_id|end_station_id|\n+----------------+----------------+--------------+\n|89E7AA6C29227EFF|             525|           660|\n|0FEFDE2603568365|             525|         16806|\n|E6159D746B2DBB91|    KA1503000012|  TA1305000029|\n|B32D3199F1C2E75B|             637|  TA1305000034|\n|83E463F23575F4BF|           13216|  TA1309000055|\n|BDAA7E3494E8D545|           18003|  KP1705001026|\n|A772742351171257|    KP1705001026|  KP1705001026|\n|295476889D9B79F8|           18003|         18003|\n|362087194BA4CC9A|    KP1705001026|  KP1705001026|\n|21630F715038CCB0|    KP1705001026|  KP1705001026|\n|A977EB7FE7F5CD3A|    KP1705001026|  KP1705001026|\n|8B868B03D6753C2A|    KP1705001026|  KP1705001026|\n|BD331D658B9D2C31|             525|           520|\n|8DFEA9BAFE6BAA62|           13253|  TA1309000050|\n|27BE9F6E67AFD86C|             525|         15578|\n|9B790D47A0A0F7F1|    KA1503000044|  KA1504000142|\n|3C2DF72600B1DE6C|    KA1503000044|  KA1504000142|\n|48A8D07ED9C7065C|    KA1503000044|  KA1504000142|\n|BBFF2AAA0A3A1A26|    KA1504000140|         17660|\n|030723CBA8CF05E7|    TA1305000032|         15542|\n+----------------+----------------+--------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Analyze based on starting and/or ending stations:\n",
    "spark.sql(\"\"\"\n",
    "    SELECT trip_id, \n",
    "           start_station_id, \n",
    "           end_station_id\n",
    "    FROM trip_fact\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ba0bb19-7c4b-4d90-a5e0-e856cbb55508",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+---------+\n|         trip_id|rider_age|\n+----------------+---------+\n|222BB8E5059252D7|       30|\n|222BB8E5059252D7|       30|\n|1826E16CB5486018|       26|\n|1826E16CB5486018|       26|\n|3D9B6A0A5330B04D|       26|\n|3D9B6A0A5330B04D|       26|\n|07E82F5E9C9E490F|       18|\n|07E82F5E9C9E490F|       18|\n|A8E94BAECBF0C2DD|       28|\n|A8E94BAECBF0C2DD|       28|\n|378F4AB323AA1D14|       28|\n|378F4AB323AA1D14|       28|\n|38AD311DC2EB1FBE|       56|\n|38AD311DC2EB1FBE|       56|\n|1D466737F0B18097|       40|\n|1D466737F0B18097|       40|\n|27E1142E1ACFAEFB|       21|\n|27E1142E1ACFAEFB|       21|\n|67F2A115DAE77924|       37|\n|67F2A115DAE77924|       37|\n+----------------+---------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Analyze based on age of the rider at the time of the ride:\n",
    "spark.sql(\"\"\"\n",
    "    SELECT trip_id, \n",
    "           TIMESTAMPDIFF(year, rider_dim.birthday, trip_fact.start_at) AS rider_age\n",
    "    FROM trip_fact\n",
    "    JOIN rider_dim ON trip_fact.rider_id = rider_dim.rider_id\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21c61aba-9419-4018-9ba3-66343934dffa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+---------+\n|         trip_id|is_member|\n+----------------+---------+\n|222BB8E5059252D7|     True|\n|222BB8E5059252D7|     True|\n|1826E16CB5486018|     True|\n|1826E16CB5486018|     True|\n|3D9B6A0A5330B04D|     True|\n|3D9B6A0A5330B04D|     True|\n|07E82F5E9C9E490F|     True|\n|07E82F5E9C9E490F|     True|\n|A8E94BAECBF0C2DD|     True|\n|A8E94BAECBF0C2DD|     True|\n|378F4AB323AA1D14|     True|\n|378F4AB323AA1D14|     True|\n|38AD311DC2EB1FBE|     True|\n|38AD311DC2EB1FBE|     True|\n|1D466737F0B18097|     True|\n|1D466737F0B18097|     True|\n|27E1142E1ACFAEFB|     True|\n|27E1142E1ACFAEFB|     True|\n|67F2A115DAE77924|     True|\n|67F2A115DAE77924|     True|\n+----------------+---------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Analyze based on rider membership status:\n",
    "spark.sql(\"\"\"\n",
    "    SELECT trip_id, \n",
    "           rider_dim.is_member\n",
    "    FROM trip_fact\n",
    "    JOIN rider_dim ON trip_fact.rider_id = rider_dim.rider_id\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c55704b2-dd17-454c-a284-4fb500f23700",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+----+------------+\n|month|quarter|year|total_amount|\n+-----+-------+----+------------+\n|    1|      1|2015|    58716.72|\n|    8|      3|2020|   748793.10|\n|   11|      4|2020|   819840.88|\n|   12|      4|2015|   103565.96|\n|    7|      3|2014|    39084.94|\n|    6|      2|2021|  1000413.30|\n|    1|      1|2021|   869782.12|\n|    3|      1|2015|    65875.08|\n|    2|      1|2014|    25169.84|\n|    6|      2|2019|   486183.00|\n|    9|      3|2020|   771152.68|\n|    3|      1|2020|   644732.74|\n|    2|      1|2013|       25.80|\n|   10|      4|2021|  1111313.20|\n|    6|      2|2018|   327605.28|\n|    1|      1|2019|   412790.68|\n|    3|      1|2019|   439194.82|\n|   10|      4|2013|    15772.24|\n|    3|      1|2013|     1635.50|\n|   11|      4|2018|   383882.86|\n+-----+-------+----+------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Analyze how much money is spent per month, quarter, and year:\n",
    "spark.sql(\"\"\"\n",
    "    SELECT EXTRACT(month FROM date) AS month,\n",
    "           EXTRACT(quarter FROM date) AS quarter,\n",
    "           EXTRACT(year FROM date) AS year,\n",
    "           SUM(amount) AS total_amount\n",
    "    FROM payment_fact\n",
    "    GROUP BY month, quarter, year\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e6be3f2-3b62-416a-aa52-d182d4b57839",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+---------------------+\n|rider_id|avg_rides_per_month|avg_minutes_per_month|\n+--------+-------------------+---------------------+\n|   70962|             22.182|              510.561|\n|   67196|              5.556|               93.759|\n|   13772|                2.0|                103.3|\n|   20868|             27.333|              709.186|\n|   64842|             26.667|               370.05|\n|   70721|               10.4|              107.277|\n|   13192|              3.143|              129.138|\n|   64595|             93.833|               3263.9|\n|   28503|             13.091|              176.685|\n|   13865|               3.75|               58.733|\n|   15271|             17.167|               312.75|\n|   29089|             14.909|              188.197|\n|   47880|             69.333|              1132.05|\n|    8304|                2.0|               135.35|\n|   68285|             13.167|              310.461|\n|   28197|             62.833|             1463.572|\n|   48063|                5.5|               77.283|\n|   39103|                2.0|                 29.5|\n|   67874|                8.8|              134.463|\n|   44446|               32.0|              522.303|\n+--------+-------------------+---------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Analyze how much money is spent per member based on average rides and minutes per month:\n",
    "spark.sql(\"\"\"\n",
    "    SELECT rider_dim.rider_id,\n",
    "           ROUND(COUNT(trip_fact.trip_id) / COUNT(DISTINCT EXTRACT(month FROM trip_fact.start_at)),3) AS avg_rides_per_month,\n",
    "           ROUND(SUM((UNIX_TIMESTAMP(trip_fact.ended_at) - UNIX_TIMESTAMP(trip_fact.start_at)) / 60) / COUNT(DISTINCT EXTRACT(month FROM trip_fact.start_at)),3) AS avg_minutes_per_month\n",
    "    FROM trip_fact\n",
    "    JOIN rider_dim ON trip_fact.rider_id = rider_dim.rider_id\n",
    "    GROUP BY rider_dim.rider_id\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e00c14bd-b87f-4bcf-a35e-ea4db63ad454",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------+------------------+\n|rider_id|rider_age_at_start|total_amount_spent|\n+--------+------------------+------------------+\n|    1028|                31|           1642.88|\n|    1197|                44|           4840.52|\n|    1588|                30|             72.00|\n|    1632|                15|            108.00|\n|    2014|                52|            540.00|\n|    2038|                48|           3096.00|\n|    2309|                27|           3024.00|\n|    2430|                40|            144.00|\n|    2443|                24|            396.00|\n|    2937|                46|           1260.00|\n|    2945|                16|            468.00|\n|    3206|                24|           1404.00|\n|    3510|                25|           1188.00|\n|    3607|                27|           2412.00|\n|    3765|                15|           3357.40|\n|    4175|                23|            756.00|\n|    4333|                26|           1656.00|\n|    4436|                26|           2556.00|\n|    4583|                24|            288.00|\n|    4875|                35|           2988.00|\n+--------+------------------+------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Analyze how much money is spent per member based on the age of the rider at account start:\n",
    "spark.sql(\"\"\"\n",
    "    SELECT rider_dim.rider_id,\n",
    "           TIMESTAMPDIFF(year, rider_dim.birthday, rider_dim.account_start_date) AS rider_age_at_start,\n",
    "           SUM(payment_fact.amount) AS total_amount_spent\n",
    "    FROM payment_fact\n",
    "    JOIN rider_dim ON payment_fact.rider_id = rider_dim.rider_id\n",
    "    GROUP BY rider_dim.rider_id, rider_age_at_start\n",
    "\"\"\").show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Project DataLakeHouse Minh Nguyen",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
